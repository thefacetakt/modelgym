{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import itertools\n",
    "import modelgym\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "from modelgym.tracker import ProgressTrackerFile\n",
    "from modelgym.trainer import Trainer\n",
    "from modelgym.util import split_and_preprocess\n",
    "from modelgym.util import TASK_CLASSIFICATION\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be trying to working with <a href=\"https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\">Breast Cancer Wisconsin (Diagnostic) Data Set</a>\n",
    "\n",
    "First, let's define some parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 20% of our dataset will be used to final evaluation and model comparisson\n",
    "TEST_SIZE = 0.2\n",
    "# ???\n",
    "N_CV_SPLITS = 3\n",
    "\n",
    "# ???\n",
    "N_ESTIMATORS = 1000\n",
    "# ???\n",
    "N_PROBES = 100         \n",
    "\n",
    "# We are using tree structured parzen estimator for hyperparameter optimization\n",
    "# Another option is 'random'\n",
    "OPTIMIZER = 'tpe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Then, let's load the dataset itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the code into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is used to transform data to inner modelgym format. For example, it would transform categorial features, if we had any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_pairs, (dtrain, dtest) = split_and_preprocess(X_train.copy(), y_train, \n",
    "                                                 X_test.copy(), y_test, \n",
    "                                                 cat_cols=[], n_splits=N_CV_SPLITS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We will train and compare <a href=\"https://xgboost.readthedocs.io/en/latest/\">xgboost</a>,\n",
    "<a href=\"https://github.com/Microsoft/LightGBM\">lightgbm</a> and <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\"> random forest </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CANDIDATES = OrderedDict([\n",
    "    ('XGBoost', modelgym.XGBModel), \n",
    "    ('LightGBM', modelgym.LGBModel),\n",
    "    ('RandomForestClassifier', modelgym.RFModel)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save intermediate results in `results` directory\n",
    "\n",
    "`config_key` will just be used in saved files names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RESULTS_DIR = \"results\"\n",
    "tracker_factory = functools.partial(ProgressTrackerFile, RESULTS_DIR, config_key='example')\n",
    "trackers = {name: tracker_factory(model_name=name) for name in CANDIDATES}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare our custom metric -- roc auc in the end of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_metric = {'roc_auc': roc_auc_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Here comes our trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(opt_evals=N_PROBES, n_estimators=N_ESTIMATORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gym_training(trainer, model, tracker, cv_pairs, dtrain, dtest, mode, custom_metric):\n",
    "    # these keys will be used later to retrieve best model from tracker\n",
    "    cv_key = mode + \"_cv\"\n",
    "    test_key = mode + \"_test\"\n",
    "    \n",
    "    if mode == \"default\":\n",
    "        # Training default model on cross validation\n",
    "        cv_metrics = trainer.crossval_fit_eval(model, cv_pairs, verbose=False)\n",
    "        \n",
    "    elif mode == \"tuned\":\n",
    "        # Tuning hyperparameters and training model on cross validation\n",
    "        cv_metrics = trainer.crossval_optimize_params(model, cv_pairs, algo_name=OPTIMIZER, tracker=tracker,\n",
    "                                                      verbose=False)\n",
    "\n",
    "    #  Saving results and (in case of tuned mode) best parameters\n",
    "    tracker._update_state({cv_key: cv_metrics})\n",
    "\n",
    "    # Fitting model on whole train data and evaluating metrics on test data\n",
    "    test_metrics = trainer.fit_eval(model, dtrain, dtest,\n",
    "                                    cv_metrics['params'],\n",
    "                                    cv_metrics['best_n_estimators'],\n",
    "                                    custom_metric=custom_metric)\n",
    "    # Saving progress\n",
    "    tracker._update_state({test_key: test_metrics})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved state to results/tracker_example_XGBoost.pickle\n",
      "saved state to results/tracker_example_XGBoost.pickle\n",
      "saved state to results/tracker_example_XGBoost.pickle\n",
      "saved state to results/tracker_example_XGBoost.pickle\n",
      "saved state to results/tracker_example_XGBoost.pickle\n",
      "saved state to results/tracker_example_XGBoost.pickle\n",
      "saved state to results/tracker_example_XGBoost.pickle\n",
      "saved state to results/tracker_example_XGBoost.pickle\n",
      "saved state to results/tracker_example_XGBoost.pickle\n",
      "saved state to results/tracker_example_XGBoost.pickle\n",
      "saved state to results/tracker_example_XGBoost.pickle\n"
     ]
    }
   ],
   "source": [
    "for model_class in CANDIDATES.values():\n",
    "    model = model_class(TASK_CLASSIFICATION)\n",
    "    name = model.get_name()\n",
    "    tracker = trackers[name]\n",
    "    gym_training(trainer, model, tracker, cv_pairs, dtrain, dtest, 'default', custom_metric)\n",
    "    gym_training(trainer, model, tracker, cv_pairs, dtrain, dtest, 'tuned', custom_metric)\n",
    "\n",
    "    trackers[name].save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare\n",
    "Let's now compare our \n",
    "\n",
    "The following cell just collects all the states of different models from trackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full_results = {}\n",
    "for i in CANDIDATES.keys():\n",
    "    if i in trackers:\n",
    "        tracker = trackers[i]\n",
    "    else:\n",
    "        tracker = tracker_factory(model_name=i)\n",
    "        tracker.load_state()\n",
    "    full_results.update({i:{'tuned': tracker.state['tuned_test'], 'default': tracker.state['default_test']}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_result(result, diff):\n",
    "    return '{:.6f} ({:+.2f}%)'.format(result, diff)\n",
    "\n",
    "def print_results(test_results, index):\n",
    "    if is_min_better:\n",
    "        baseline = test_results.min()\n",
    "    else:\n",
    "        baseline = test_results.max()\n",
    "    diff = 100 * test_results / baseline - 100\n",
    "    test_results_formatted = \\\n",
    "        [[format_result(test_results[i, j], diff[i, j]) for j in range(2)] for i in range(len(index))]\n",
    "\n",
    "    print(pd.DataFrame(test_results_formatted, columns=['default', 'tuned'], index=index))\n",
    "    \n",
    "def plot_results(test_results, index):\n",
    "    full_names = [\"%s %s\" % (name, mode) for name in index for mode in ['default', 'tuned']]\n",
    "    named_results = zip(full_names, test_results.flatten())\n",
    "\n",
    "    sorted_results = sorted(named_results, key=lambda x: x[1], reverse=not is_min_better)\n",
    "    \n",
    "    xticks = ['%s\\n%.5f' % (name, loss) for name, loss in sorted_results]\n",
    "\n",
    "    pyplot.figure(figsize=(20, 7))\n",
    "    pyplot.scatter(range(len(full_names)), list(zip(*sorted_results))[1], s=150)\n",
    "    pyplot.xticks(range(len(full_names)), xticks, fontsize=15)\n",
    "    pyplot.yticks(fontsize=12)\n",
    "    pyplot.title('Comparison', fontsize=20)\n",
    "    pyplot.ylabel(metric, fontsize=16)\n",
    "\n",
    "def view_results(full_results, index, metric, is_min_better=True):\n",
    "    test_results_list = [[result['default'][metric], result['tuned'][metric]] for result in full_results.values()]\n",
    "    test_results = np.array(test_results_list)\n",
    "    print_results(test_results, index)\n",
    "    plot_results(test_results, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline --no-import-all\n",
    "\n",
    "metric = 'roc_auc'\n",
    "is_min_better = False\n",
    "\n",
    "view_results(full_results, CANDIDATES.keys(), metric, is_min_better=is_min_better)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models with stat test\n",
    "\n",
    "We clearly see, that some models are better, than another. But is this difference significant or random?\n",
    "\n",
    "Let's use the power of statistics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare that out tuned RF model difference from default is statistically significant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_rf_model = full_results['RandomForestClassifier']['default']['bst']\n",
    "tuned_rf_model = full_results['RandomForestClassifier']['tuned']['bst']\n",
    "\n",
    "print('Two models are different: {}, p-value {}'.format(\n",
    "    *modelgym.util.compare_models_different(tuned_rf_model, default_rf_model, dtest)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Compare default LightGBM and default RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_lightGBM_model = full_results['LightGBM']['default']['bst']\n",
    "\n",
    "print('Two models are different: {}, p-value {}'.format(\n",
    "    *modelgym.util.compare_models_different(default_lightGBM_model, default_rf_model, dtest)\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
